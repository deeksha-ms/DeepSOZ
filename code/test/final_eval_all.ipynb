{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0801b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deeksha/anaconda3/envs/deepai/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy  \n",
    "from visualization import * \n",
    "import torch.nn as nn\n",
    "from utils import *\n",
    "from dataloader import *\n",
    "\n",
    "import sklearn.metrics as metrics \n",
    "\n",
    "from txlstm_szpool import *\n",
    "from baselines import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2aa7cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#edit as required\n",
    "\n",
    "device = 'cpu'\n",
    "data_root = '/home/deeksha/EEG_Sz/miccai23/'\n",
    "manifest = read_manifest('/home/deeksha/EEG_Sz/miccai23/'+ 'data/tuh_single_windowed_manifest.csv', ',')\n",
    "\n",
    "\n",
    "n=15\n",
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55627604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_metrics(dataloader, model):\n",
    "    model.eval()\n",
    "    running_loss = 0.0 \n",
    "    running_corrects = 0.0\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    positives = 0\n",
    "    length = 0\n",
    "    detloss = nn.CrossEntropyLoss()\n",
    "    trues = []\n",
    "    proba = []\n",
    "    for batch_idx, data in enumerate(dataloader): \n",
    "        \n",
    "        inputs = data['buffers']\n",
    "        labels = data['sz_labels']\n",
    "        labels = labels.reshape(-1).long()\n",
    "   \n",
    "        inputs = inputs.to(torch.DoubleTensor())\n",
    "        chn_output, _, _ = model(inputs)[:3]\n",
    "        outputs = chn_output.reshape(-1,2)\n",
    "        p = F.softmax(outputs, dim=-1).detach().cpu().numpy()[:, 1]\n",
    "        proba.append(p)\n",
    "        trues.append(labels.detach().cpu().numpy().reshape(-1))\n",
    "        del inputs\n",
    "        \n",
    "       \n",
    "        pred = torch.argmax(outputs.data, 1).long()  #cross entropy\n",
    "        \n",
    "        length += (pred.shape)[0]\n",
    "        running_corrects += pred.eq(labels.data.view_as(pred)).cpu().sum()\n",
    "        true_positives += ((pred==1)&(labels.data.view_as(pred)==1)).cpu().sum()   \n",
    "        true_negatives += ((pred==0)&(labels.data.view_as(pred)==0)).cpu().sum()  \n",
    "        positives += labels.data.view_as(pred).cpu().sum()\n",
    "\n",
    "        del pred, labels\n",
    "        torch.cuda.empty_cache()\n",
    "    del model   \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    acc = round(running_corrects.item()/ length, 3)\n",
    "    sens = round(true_positives.item()/positives.item() , 3)\n",
    "    spec = round(true_negatives.item()/ (length-positives.item()), 3)\n",
    "    trues = np.concatenate(trues)\n",
    "    proba = np.concatenate(proba)\n",
    "    auc = metrics.roc_auc_score(trues, proba)\n",
    "    return epoch_loss, acc, sens, spec, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52001d88",
   "metadata": {},
   "source": [
    "# Detection results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7a4e16",
   "metadata": {},
   "source": [
    "## Window level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5f6f83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detmodels = {'txlstm_szpool':{'auc': [], 'sens':[], 'spec':[]},\n",
    "             'txlstm_maxpool':{'auc': [], 'sens':[], 'spec':[]},\n",
    "             'tgcn_szpool':{'auc': [], 'sens':[], 'spec':[]} ,\n",
    "             'sztrack_szpool':{'auc': [], 'sens':[], 'spec':[]}, \n",
    "             'cnnblstm':{'auc': [], 'sens':[], 'spec':[]}\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6892545",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = []\n",
    "for cvfold in range(0,15,1):\n",
    "    foldroot = 'final_models/fold'+str(cvfold)+'/'\n",
    "    allfiles = os.listdir(foldroot)\n",
    "    modelfiles = os.listdir(foldroot)\n",
    "    testfile = list(filter(lambda f:f.startswith('pts_test'), allfiles))[0]\n",
    "    \n",
    "    val_pts = np.load(foldroot+testfile)\n",
    "    val_set =  pretrainLoader(data_root, val_pts, manifest, addNoise=False, \n",
    "                                          input_mask=None, ablate=False, permute=False, normalize=True)\n",
    "    validation_loader = DataLoader(val_set, batch_size=1, shuffle=True)\n",
    "                \n",
    "    \n",
    "    for modelname in list(detmodels.keys()):\n",
    "        print(cvfold, modelname)\n",
    "        \n",
    "        \n",
    "        statedict = torch.load(foldroot+modelfile)\n",
    "        \n",
    "        if modelname == 'cnnblstm':\n",
    "            modelfile = list(filter(lambda f:f.startswith(modelname+'_'), modelfiles))[0]\n",
    "            statedict = torch.load(foldroot+modelfile)\n",
    "            model = CNN_BLSTM()\n",
    "            model.load_state_dict(statedict)\n",
    "      \n",
    "        elif modelname == 'txlstm_maxpool':\n",
    "            modelfile = list(filter(lambda f:f.startswith(modelname+'_fine'), modelfiles))[0]\n",
    "            statedict = torch.load(foldroot+modelfile)\n",
    "            prefn =foldroot+list(filter(lambda f:f.startswith('txlstm'+'_pre'), allfiles))[0]\n",
    "            model = txlstm_szpool(transformer_dropout=0.15, \n",
    "                                  pretrained = prefn, \n",
    "                                  modelname = 'txlstm', pooltype='maxpool')\n",
    "            model.load_state_dict(statedict)\n",
    "\n",
    "        elif modelname == 'tgcn_szpool':\n",
    "            modelfile = list(filter(lambda f:f.startswith(modelname+'_fine'), modelfiles))[0]\n",
    "            statedict = torch.load(foldroot+modelfile)\n",
    "            prefn = foldroot+list(filter(lambda f:f.startswith('tgcn'+'_pre'), allfiles))[0]\n",
    "            model = txlstm_szpool(transformer_dropout=0.15, \n",
    "                                  pretrained = prefn, \n",
    "                                  modelname = 'tgcn', pooltype='szpool')\n",
    "            model.load_state_dict(statedict)\n",
    "        \n",
    "        elif modelname =='sztrack_szpool':\n",
    "            modelfile = list(filter(lambda f:f.startswith(modelname+'_fine'), modelfiles))[0]\n",
    "            statedict = torch.load(foldroot+modelfile)            \n",
    "                model = sztrack()\n",
    "                model.load_state_dict(statedict)\n",
    "        else:\n",
    "            modelfile = list(filter(lambda f:f.startswith(modelname+'_fine'), modelfiles))[0]\n",
    "            statedict = torch.load(foldroot+modelfile)            \n",
    "            prefn = foldroot+list(filter(lambda f:f.startswith('txlstm'+'_pre'), allfiles))[0]\n",
    "            model = txlstm_szpool(transformer_dropout=0.15, \n",
    "                                  pretrained = prefn, \n",
    "                                  modelname = 'txlstm', pooltype='szpool')\n",
    "            model.load_state_dict(statedict)\n",
    "    \n",
    "        model.double()\n",
    "        loss, acc, sens, spec, auc = detection_metrics(validation_loader, model)\n",
    "        \n",
    "        detmodels[modelname]['auc'].append(auc)\n",
    "        detmodels[modelname]['sens'].append(sens)\n",
    "        detmodels[modelname]['spec'].append(spec)\n",
    "        \n",
    "        del model\n",
    "    #break\n",
    "    del validation_loader\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2830c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6559430b",
   "metadata": {},
   "source": [
    "## seizure level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6be3a7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ablate = {'sztrack_szpool':{'fpr': [], 'sens':[], 'lat':[]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6c7caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingAverage(nn.Module):\n",
    "    def __init__(self, winlen = 21):\n",
    "        super(MovingAverage,self).__init__()\n",
    "        self.winlen = winlen\n",
    "        self.layer = nn.AvgPool1d(kernel_size=winlen, stride=1, padding=int((winlen-1)/2), count_include_pad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "    \n",
    "    \n",
    "sig = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e73d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresrange = np.arange(0.3, 0.75, 0.05)\n",
    "smoother = MovingAverage(31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d4f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cvfold in range(0,15,1):\n",
    "    foldroot = 'final_models/fold'+str(cvfold)+'/'\n",
    "    allfiles = os.listdir(foldroot)\n",
    "    #modelfiles = list(filter(lambda f:f.endswith('.tar'), allfiles))\n",
    "    modelfiles = os.listdir(foldroot+'models/')\n",
    "    testfile = list(filter(lambda f:f.startswith('pts_test'), allfiles))[0]\n",
    "    \n",
    "    val_pts = np.load(foldroot+testfile)\n",
    "    val_set =  pretrainLoader(data_root, val_pts, manifest, addNoise=False, \n",
    "                                          input_mask=None, ablate=False, permute=False, normalize=True)\n",
    "    test_loader = DataLoader(val_set, batch_size=1, shuffle=True)\n",
    "                \n",
    "    \n",
    "        \n",
    "    for modelname in list(detmodels.keys()):\n",
    "        print(cvfold, modelname)\n",
    "        \n",
    "        \n",
    "        statedict = torch.load(foldroot+modelfile)\n",
    "        \n",
    "        if modelname == 'cnnblstm':\n",
    "            modelfile = list(filter(lambda f:f.startswith(modelname+'_'), modelfiles))[0]\n",
    "            statedict = torch.load(foldroot+modelfile)\n",
    "            model = CNN_BLSTM()\n",
    "            model.load_state_dict(statedict)\n",
    "      \n",
    "        elif modelname == 'txlstm_maxpool':\n",
    "            modelfile = list(filter(lambda f:f.startswith(modelname+'_fine'), modelfiles))[0]\n",
    "            statedict = torch.load(foldroot+modelfile)\n",
    "            prefn =foldroot+list(filter(lambda f:f.startswith('txlstm'+'_pre'), allfiles))[0]\n",
    "            model = txlstm_szpool(transformer_dropout=0.15, \n",
    "                                  pretrained = prefn, \n",
    "                                  modelname = 'txlstm', pooltype='maxpool')\n",
    "            model.load_state_dict(statedict)\n",
    "\n",
    "        elif modelname == 'tgcn_szpool':\n",
    "            modelfile = list(filter(lambda f:f.startswith(modelname+'_fine'), modelfiles))[0]\n",
    "            statedict = torch.load(foldroot+modelfile)\n",
    "            prefn = foldroot+list(filter(lambda f:f.startswith('tgcn'+'_pre'), allfiles))[0]\n",
    "            model = txlstm_szpool(transformer_dropout=0.15, \n",
    "                                  pretrained = prefn, \n",
    "                                  modelname = 'tgcn', pooltype='szpool')\n",
    "            model.load_state_dict(statedict)\n",
    "        \n",
    "        elif modelname =='sztrack_szpool':\n",
    "            modelfile = list(filter(lambda f:f.startswith(modelname+'_fine'), modelfiles))[0]\n",
    "            statedict = torch.load(foldroot+modelfile)            \n",
    "                model = sztrack()\n",
    "                model.load_state_dict(statedict)\n",
    "        else:\n",
    "            modelfile = list(filter(lambda f:f.startswith(modelname+'_fine'), modelfiles))[0]\n",
    "            statedict = torch.load(foldroot+modelfile)            \n",
    "            prefn = foldroot+list(filter(lambda f:f.startswith('txlstm'+'_pre'), allfiles))[0]\n",
    "            model = txlstm_szpool(transformer_dropout=0.15, \n",
    "                                  pretrained = prefn, \n",
    "                                  modelname = 'txlstm', pooltype='szpool')\n",
    "            model.load_state_dict(statedict)\n",
    "    \n",
    "        model.double()\n",
    "        final_thres = 0.2\n",
    "        \n",
    "        for j, thres in enumerate(thresrange):\n",
    "            fp_count = 0\n",
    "            time = 0\n",
    "            for data in validation_loader:\n",
    "                x = data['buffers'].double()\n",
    "                ytrue = data['sz_labels'].reshape(-1).detach().cpu()\n",
    "                nsz = x.shape[1]\n",
    "                y, _, _ = model(x)[:3]\n",
    "                proba = F.softmax(y.reshape(-1, 2), -1)[:, 1].reshape(nsz, -1)\n",
    "                p_smooth = smoother(proba.detach().cpu()[None, :, :])[:, ].reshape(-1)\n",
    "                ypred = torch.zeros_like(ytrue)\n",
    "                ypred[p_smooth >= thres] = 1 \n",
    "                \n",
    "                time += nsz*600 / 3600\n",
    "                fp_count += (ypred[ytrue==0]==1).sum()\n",
    "            fpr = fp_count/time\n",
    "            if fpr <= 120:\n",
    "                final_thres = thres\n",
    "                break\n",
    "\n",
    "\n",
    "        fp_count, time = 0, 0\n",
    "        latency = []\n",
    "        tp = 0 \n",
    "        p = 0\n",
    "        FPM = []\n",
    "\n",
    "        time_control = 0\n",
    "        for data in test_loader:\n",
    "            x = data['buffers'][:, :1].double()\n",
    "            ytrue = data['sz_labels'][:, :1].reshape(-1).detach().numpy()\n",
    "            nsz = x.shape[1]\n",
    "            y, _, _ = model(x)[:3]\n",
    "            proba = F.softmax(y.reshape(-1, 2), -1)[:, 1].reshape(nsz, -1)\n",
    "            p_smooth = smoother(proba.detach().cpu()[None, :, :])[:, ].reshape(-1).detach().numpy()\n",
    "            ypred = np.zeros_like(ytrue)\n",
    "            ypred[p_smooth > final_thres] = 1 \n",
    "            \n",
    "            ypred = ypred.reshape(nsz, -1)\n",
    "            p_smooth = p_smooth.reshape(nsz, -1)\n",
    "\n",
    "            \n",
    "           \n",
    "            p += nsz\n",
    "            ytrue = ytrue.reshape(nsz, -1)\n",
    "                                 \n",
    "            time += nsz*600 / 3600\n",
    "            ton = np.argmax(ytrue, 1)\n",
    "            tonpred = np.argmax(ypred, 1)\n",
    "            for j in range(nsz):\n",
    "                s = max(0, ton[j] - 10)\n",
    "                time_control += s+1\n",
    "                if s==0:\n",
    "                    s = 1\n",
    "                temp = ypred[j, :s] - np.concatenate(([0], ypred[j, : s-1]))\n",
    "                fp_count += (temp==1).sum()\n",
    "                ysz = ypred[j]\n",
    "                tp += int((ysz[ytrue[j]==1]==1).any())\n",
    "            \n",
    "                \n",
    "                temp = ypred[j, ] - np.concatenate(([0], ypred[j, : -1]))\n",
    "                onsets = np.where(temp==1)[0]\n",
    "                offsets = np.where(temp==-1)[0]\n",
    "                if not len(onsets) == 0:\n",
    "                    \n",
    "                    for eno in range(len(onsets)):\n",
    "                        try: \n",
    "                            e = offsets[eno] \n",
    "                        except:\n",
    "                            e = -1\n",
    "                        if (ytrue[j, onsets[eno]:e] == 1).any():\n",
    "                            break\n",
    "                    latency.append(onsets[eno] - ton[j])\n",
    "           \n",
    "                fpm = ypred[j, :s].sum()/60\n",
    "                FPM.append(fpm)\n",
    "        \n",
    "        del model\n",
    "        \n",
    "        detmodels[modelname]['fpr'].append( np.mean(FPM))\n",
    "        detmodels[modelname]['sens'].append(tp/p)\n",
    "        detmodels[modelname]['lat'].append(np.mean(latency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "182ca8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0665415579802517, 0.8443198665507652)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.mean(ablate['sztrack_szpool']['fpr']), np.std(ablate['sztrack_szpool']['fpr']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fe2776",
   "metadata": {},
   "source": [
    "## Delong test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b780b1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delong import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db480777",
   "metadata": {},
   "outputs": [],
   "source": [
    "delongmodels = ['txlstm_szpool', 'cnnblstm', 'tgcn_szpool', 'txlstm_maxpool', 'sztrack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2eccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for cvfold in range(0,15,1):\n",
    "    foldroot = 'final_models/fold'+str(cvfold)+'/'\n",
    "    allfiles = os.listdir(foldroot)\n",
    "    modelfiles = list(filter(lambda f:f.endswith('.tar'), allfiles))\n",
    "    testfile = list(filter(lambda f:f.startswith('pts_test'), allfiles))[0]\n",
    "    \n",
    "    val_pts = np.load(foldroot+testfile)\n",
    "    val_set =  pretrainLoader(data_root, val_pts, manifest, addNoise=False, \n",
    "                                          input_mask=None, ablate=False, permute=False, normalize=True)\n",
    "    test_loader = DataLoader(val_set, batch_size=1, shuffle=True)\n",
    "                \n",
    "    ground = []\n",
    "    pred_txlstm = []\n",
    "    pred_tgcn = []\n",
    "    pred_cnnblstm = []\n",
    "    pred_max = []\n",
    "    pred_sztrack = []\n",
    "    for modelname in list(delongmodels):\n",
    "   \n",
    "        \n",
    "        if modelname == 'cnnblstm':\n",
    "                modelfile = list(filter(lambda f:f.startswith(modelname+'_pret'), allfiles))[0]\n",
    "                statedict = torch.load(foldroot+modelfile)\n",
    "                model_cnnblstm = CNN_BLSTM()\n",
    "                model_cnnblstm.load_state_dict(statedict)\n",
    "                model_cnnblstm.double()\n",
    "\n",
    "        elif modelname == 'tgcn_szpool':\n",
    "                modelfile = list(filter(lambda f:f.startswith(modelname+'_fine'), allfiles))[0]\n",
    "                statedict = torch.load(foldroot+modelfile)\n",
    "                prefn = foldroot+list(filter(lambda f:f.startswith('tgcn'+'_pre'), allfiles))[0]\n",
    "                model_tgcn = txlstm_szpool(transformer_dropout=0.1, \n",
    "                                  pretrained = prefn, \n",
    "                                  modelname = 'tgcn', pooltype='szpool')\n",
    "                model_tgcn.load_state_dict(statedict)\n",
    "                model_tgcn.double()\n",
    "                \n",
    "        elif modelname =='sztrack_szpool':\n",
    "                modelfile = list(filter(lambda f:f.startswith(modelname+'_fine'), modelfiles))[0]\n",
    "                statedict = torch.load(foldroot+modelfile)            \n",
    "                model_sztrack = sztrack()\n",
    "                model_sztrack.load_state_dict(statedict)\n",
    "                \n",
    "        elif modelname == 'txlstm_maxpool':\n",
    "                modelfile = list(filter(lambda f:f.startswith(modelname+'_fine'), allfiles))[0]\n",
    "                statedict = torch.load(foldroot+modelfile)\n",
    "                prefn = foldroot+list(filter(lambda f:f.startswith('txlstm'+'_pre'), allfiles))[0]\n",
    "                model_max = txlstm_szpool(transformer_dropout=0.1, \n",
    "                                  pretrained = prefn, \n",
    "                                  modelname = 'txlstm', pooltype='szpool')\n",
    "                model_max.load_state_dict(statedict)\n",
    "                model_max.double()         \n",
    "        \n",
    "        else:\n",
    "                modelfile = list(filter(lambda f:f.startswith(modelname+'_fine'), allfiles))[0]\n",
    "                statedict = torch.load(foldroot+modelfile)\n",
    "                prefn = foldroot+list(filter(lambda f:f.startswith('txlstm'+'_pre'), allfiles))[0]\n",
    "                model_txlstm = txlstm_szpool(transformer_dropout=0.1, \n",
    "                                  pretrained = prefn, \n",
    "                                  modelname = 'txlstm', pooltype='szpool')\n",
    "                model_txlstm.load_state_dict(statedict)\n",
    "                model_txlstm.double()\n",
    "        \n",
    "    model_txlstm.eval()\n",
    "    model_tgcn.eval()\n",
    "    model_max.eval()\n",
    "    model_cnnblstm.eval()\n",
    "    model_sztrack.eval()\n",
    "    for idx, data in enumerate(test_loader):\n",
    "            x = data['buffers'].double()\n",
    "            ytrue = data['sz_labels'].reshape(-1).detach().numpy()\n",
    "            nsz = x.shape[1]\n",
    "            \n",
    "           \n",
    "            y, _, _ = model_txlstm(x)[:3]\n",
    "            pred_txlstm.append(F.softmax(y.reshape(-1, 2), -1)[:, 1].reshape(-1).detach().numpy())\n",
    "            del y\n",
    "        \n",
    "            \n",
    "            \n",
    "            y, _, _ = model_tgcn(x)[:3]\n",
    "            pred_tgcn.append(F.softmax(y.reshape(-1, 2), -1)[:, 1].reshape(-1).detach().numpy())\n",
    "            del y\n",
    "            \n",
    "            \n",
    "            y, _, _ = model_cnnblstm(x)[:3]\n",
    "            pred_cnnblstm.append(F.softmax(y.reshape(-1, 2), -1)[:, 1].reshape(-1).detach().numpy())\n",
    "            del y\n",
    "           \n",
    "            y, _, _ = model_sztrack(x)[:3]\n",
    "            pred_sztrack.append(F.softmax(y.reshape(-1, 2), -1)[:, 1].reshape(-1).detach().numpy())\n",
    "            del y\n",
    "           \n",
    "            \n",
    "            y, _, _ = model_max(x)[:3]\n",
    "            pred_max.append(F.softmax(y.reshape(-1, 2), -1)[:, 1].reshape(-1).detach().numpy())\n",
    "            del y\n",
    "            \n",
    "            ground.append(ytrue)\n",
    "            \n",
    "\n",
    "            #if idx==5:\n",
    "            #    break\n",
    "    del model_txlstm, model_tgcn,model_cnnblstm, model_sztrack\n",
    "    pred_txlstm = np.concatenate(pred_txlstm)\n",
    "    pred_sztrack = np.concatenate(pred_sztrack)\n",
    "    pred_tgcn = np.concatenate(pred_tgcn)\n",
    "    pred_cnnblstm = np.concatenate(pred_cnnblstm)\n",
    "    pred_max = np.concatenate(pred_max)\n",
    "  \n",
    "    ground = np.concatenate(ground)\n",
    "    print('tgcn', np.round(10**delong_roc_test(ground, pred_txlstm, pred_tgcn), 3))\n",
    "    print('cnnblstm', np.round( 10**delong_roc_test(ground, pred_txlstm, pred_cnnblstm), 3))\n",
    "    print('sztrack', np.round( 10**delong_roc_test(ground, pred_txlstm, pred_sztrack), 3))\n",
    "    print('txlstm_max', np.round( 10**delong_roc_test(ground, pred_txlstm, pred_max), 3))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab2cd24",
   "metadata": {},
   "source": [
    "# Localization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4332479",
   "metadata": {},
   "outputs": [],
   "source": [
    "chn_neighbours = {0: [1,2,3,4], \n",
    "                  1: [0,4,5,6], \n",
    "                  2: [0,3,4,7,8], \n",
    "                  3: [0,2,4,8,9], \n",
    "                  4: [0,1,3,5,9], \n",
    "                  5: [1,4,6,9,10],\n",
    "                  6: [1,4,5,10,11], \n",
    "                  7: [2,8,12,13, 17], \n",
    "                  8: [2,3,4,7,9,12,13,14], \n",
    "                  9: [3,4,5,8,10,13,14,15], \n",
    "                 10: [4,5,6,9,11,14,15,16], \n",
    "                 11: [6, 10, 15, 16, 18], \n",
    "                 12: [7, 8, 13, 17], \n",
    "                 13: [7, 8, 9, 12, 14, 17],\n",
    "                 14: [8,9,10,13,15,17,18],\n",
    "                 15: [9,10,11,14,16,18], \n",
    "                 16: [10,11,15,18], \n",
    "                 17: [7,12,13,14,18], \n",
    "                 18: [11, 14,15, 16, 17]}\n",
    "\n",
    "def check_neighborhood(max_chn, onset_map):\n",
    "    check = False\n",
    "    for i in range(19):\n",
    "        if onset_map[i]==1:\n",
    "            surrounding = chn_neighbours[i]\n",
    "            if max_chn in surrounding:\n",
    "                check = True\n",
    "    return check\n",
    "\n",
    "def final_loc(psoz, true_onset):\n",
    "            n = psoz.shape[0]\n",
    "            m = psoz.max(1).reshape(n,1)\n",
    "            psoz = psoz/m\n",
    "            ysoz = psoz.mean(0)\n",
    "            #ysoz /= ysoz.max()\n",
    "            max_chn_loc = np.argmax(ysoz)                    \n",
    "            max_chn_correct = 1 if true_onset[max_chn_loc]==1 else 0\n",
    "            \n",
    "            \n",
    "            if check_neighborhood(max_chn_loc, true_onset) and true_onset.sum()<=4:\n",
    "                        max_chn_correct =1 \n",
    "                       \n",
    " \n",
    "  \n",
    "            Uev = psoz.var(0)\n",
    "\n",
    "            return ysoz, Uev, max_chn_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6193932c",
   "metadata": {},
   "source": [
    "## patient level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "329d3794",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "locmodels = {'txlstm_szpool':{'auc': [], 'sens':[], 'spec':[]},\n",
    "             'txlstm_maxpool':{'auc': [], 'sens':[], 'spec':[]},\n",
    "             'tgcn_szpool':{'auc': [], 'sens':[], 'spec':[]} ,\n",
    "             'sztrack_szpool':{'auc': [], 'sens':[], 'spec':[]}\n",
    "            \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47052e26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cvfold in range(5,6,1):\n",
    "    foldroot = 'final_models/fold'+str(cvfold)+'/'\n",
    "    allfiles = os.listdir(foldroot)\n",
    "    #modelfiles = list(filter(lambda f:f.endswith('.tar'), allfiles))\n",
    "    modelfiles = os.listdir(foldroot)\n",
    "    testfile = list(filter(lambda f:f.startswith('pts_test'), allfiles))[0]\n",
    "    \n",
    "    val_pts = np.load(foldroot+testfile)\n",
    "   \n",
    "  \n",
    "    for modelname in list(locmodels.keys()):\n",
    "        print(cvfold, modelname)\n",
    "        \n",
    "        \n",
    "        statedict = torch.load(foldroot+modelfile)\n",
    "        \n",
    "\n",
    "      \n",
    "        if modelname == 'txlstm_maxpool':\n",
    "            modelfile = list(filter(lambda f:f.startswith(modelname+'_fine'), modelfiles))[0]\n",
    "            statedict = torch.load(foldroot+modelfile)\n",
    "            prefn =foldroot+list(filter(lambda f:f.startswith('txlstm'+'_pre'), allfiles))[0]\n",
    "            model = txlstm_szpool(transformer_dropout=0.15, \n",
    "                                  pretrained = prefn, \n",
    "                                  modelname = 'txlstm', pooltype='maxpool')\n",
    "            model.load_state_dict(statedict)\n",
    "\n",
    "        elif modelname == 'tgcn_szpool':\n",
    "            modelfile = list(filter(lambda f:f.startswith(modelname+'_fine'), modelfiles))[0]\n",
    "            statedict = torch.load(foldroot+modelfile)\n",
    "            prefn = foldroot+list(filter(lambda f:f.startswith('tgcn'+'_pre'), allfiles))[0]\n",
    "            model = txlstm_szpool(transformer_dropout=0.15, \n",
    "                                  pretrained = prefn, \n",
    "                                  modelname = 'tgcn', pooltype='szpool')\n",
    "            model.load_state_dict(statedict)\n",
    "        \n",
    "        elif modelname =='sztrack_szpool':\n",
    "            modelfile = list(filter(lambda f:f.startswith(modelname+'_fine'), modelfiles))[0]\n",
    "            statedict = torch.load(foldroot+modelfile)            \n",
    "                model = sztrack()\n",
    "                model.load_state_dict(statedict)\n",
    "        else:\n",
    "            modelfile = list(filter(lambda f:f.startswith(modelname+'_fine'), modelfiles))[0]\n",
    "            statedict = torch.load(foldroot+modelfile)            \n",
    "            prefn = foldroot+list(filter(lambda f:f.startswith('txlstm'+'_pre'), allfiles))[0]\n",
    "            model = txlstm_szpool(transformer_dropout=0.15, \n",
    "                                  pretrained = prefn, \n",
    "                                  modelname = 'txlstm', pooltype='szpool')\n",
    "            model.load_state_dict(statedict)\n",
    "       \n",
    "        model.double()\n",
    "        corr_pt = 0\n",
    "        uncs = []\n",
    "        for pt in val_pts:\n",
    "            \n",
    "            sets =  pretrainLoader(data_root, [pt], manifest, addNoise=False, normalize=True, \n",
    "                                   input_mask=None, ablate=False, permute=False)\n",
    "            loader = DataLoader(sets, batch_size=1, shuffle=True)\n",
    "            \n",
    "            \n",
    "            correct_loc = 0\n",
    "            tot_sz = 0\n",
    "            ysoz_all = []\n",
    "            a_all = []\n",
    "      \n",
    "            for idx, data in enumerate(loader):\n",
    "                    \n",
    "                    inputs = data['buffers'].double()\n",
    "                    B,Nsz,T, C, N = inputs.shape\n",
    "                    chn_pos = torch.arange(C)\n",
    "                    true_onset = data['onset map'].detach().numpy().reshape(-1)\n",
    "    \n",
    "                    output, psoz, _, a = model(inputs)\n",
    "                    ysoz_all.append(psoz.reshape(Nsz, C).detach().cpu().numpy())\n",
    "                    a_all.append(a)\n",
    "                    tot_sz += Nsz\n",
    "                    \n",
    "            \n",
    "           \n",
    "            ysoz_all = np.concatenate(ysoz_all).reshape(tot_sz, 19)\n",
    "            psoz = ysoz_all\n",
    "            ysoz, conf_y, max_chn_correct_y = final_loc(psoz, true_onset)\n",
    "\n",
    "            \n",
    "            corr_pt += max_chn_correct_y\n",
    "            \n",
    "            uncs.append(conf_y)\n",
    "            #f, ax = plt.subplots(1,1, figsize = (4,4))\n",
    "            \n",
    "            #pos2d = topoplot(ysoz, ax,\n",
    "            #                    title=pt, zone=1,\n",
    "            #                   lobe_correct=max_chn_correct_y,\n",
    "            #                   lat_correct=max_chn_correct_y,\n",
    "            #                  onset_map=true_onset)\n",
    "            \n",
    "            #plt.show()\n",
    "        print(modelname, cvfold, corr_pt, np.array(uncs).mean() )\n",
    "        del model\n",
    "        \n",
    "        locmodels[modelname]['ptcorr'].append(corr_pt)\n",
    "        locmodels[modelname]['ptunc'].append(np.array(uncs).max(1).mean())\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "145a3f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonf = json.dumps(locmodels)\n",
    "\n",
    "# open file for writing, \"w\" \n",
    "f = open(\"loc_finetuned_finalresults_nomask.json\",\"w\")\n",
    "\n",
    "# write json object to file\n",
    "f.write(jsonf)\n",
    "\n",
    "# close file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1b9e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"loc_finetuned_finalresults.json\") as json_file:\n",
    "    temp = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3fb2c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6930416666666667 0.10675\n",
      "0.411125 0.075875\n",
      "0.486125 0.12329166666666667\n",
      "0.6805416666666666 0.082875\n",
      "0.6555416666666667 0.11020833333333334\n"
     ]
    }
   ],
   "source": [
    "for m in list(temp.keys()):\n",
    "\n",
    "    print(np.round(np.mean(temp[m]['ptcorr']), 3)/24, np.round(np.std(temp[m]['ptcorr']), 3)/24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334f7bfb",
   "metadata": {},
   "source": [
    "## sz level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd184b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples =20\n",
    "for cvfold in range(0,15,1):\n",
    "    foldroot = 'final_models/fold'+str(cvfold)+'/'\n",
    "    allfiles = os.listdir(foldroot)\n",
    "    #modelfiles = list(filter(lambda f:f.endswith('.tar'), allfiles))\n",
    "    modelfiles = os.listdir(foldroot+'/txmodels/')\n",
    "    testfile = list(filter(lambda f:f.startswith('pts_test'), allfiles))[0]\n",
    "    val_pts = np.load(foldroot+testfile)\n",
    "    sets =  pretrainLoader(data_root, val_pts, manifest, addNoise=False, normalize=True, \n",
    "                                   input_mask=None, ablate=False, permute=False)\n",
    "    test_loader = DataLoader(sets, batch_size=1, shuffle=True)\n",
    "    \n",
    "    val_pts = np.load(foldroot+testfile)\n",
    "   \n",
    "  \n",
    "        for modelname in list(locmodels.keys()):\n",
    "        print(cvfold, modelname)\n",
    "        \n",
    "        \n",
    "        statedict = torch.load(foldroot+modelfile)\n",
    "        \n",
    "\n",
    "      \n",
    "        if modelname == 'txlstm_maxpool':\n",
    "            modelfile = list(filter(lambda f:f.startswith(modelname+'_fine'), modelfiles))[0]\n",
    "            statedict = torch.load(foldroot+modelfile)\n",
    "            prefn =foldroot+list(filter(lambda f:f.startswith('txlstm'+'_pre'), allfiles))[0]\n",
    "            model = txlstm_szpool(transformer_dropout=0.15, \n",
    "                                  pretrained = prefn, \n",
    "                                  modelname = 'txlstm', pooltype='maxpool')\n",
    "            model.load_state_dict(statedict)\n",
    "\n",
    "        elif modelname == 'tgcn_szpool':\n",
    "            modelfile = list(filter(lambda f:f.startswith(modelname+'_fine'), modelfiles))[0]\n",
    "            statedict = torch.load(foldroot+modelfile)\n",
    "            prefn = foldroot+list(filter(lambda f:f.startswith('tgcn'+'_pre'), allfiles))[0]\n",
    "            model = txlstm_szpool(transformer_dropout=0.15, \n",
    "                                  pretrained = prefn, \n",
    "                                  modelname = 'tgcn', pooltype='szpool')\n",
    "            model.load_state_dict(statedict)\n",
    "        \n",
    "        elif modelname =='sztrack_szpool':\n",
    "            modelfile = list(filter(lambda f:f.startswith(modelname+'_fine'), modelfiles))[0]\n",
    "            statedict = torch.load(foldroot+modelfile)            \n",
    "                model = sztrack()\n",
    "                model.load_state_dict(statedict)\n",
    "        else:\n",
    "            modelfile = list(filter(lambda f:f.startswith(modelname+'_fine'), modelfiles))[0]\n",
    "            statedict = torch.load(foldroot+modelfile)            \n",
    "            prefn = foldroot+list(filter(lambda f:f.startswith('txlstm'+'_pre'), allfiles))[0]\n",
    "            model = txlstm_szpool(transformer_dropout=0.15, \n",
    "                                  pretrained = prefn, \n",
    "                                  modelname = 'txlstm', pooltype='szpool')\n",
    "            model.load\n",
    "        \n",
    "        del statedict \n",
    "        corr_sz = []\n",
    "        mc_var = []\n",
    "        \n",
    "        for idx, data in enumerate(test_loader):\n",
    "\n",
    "            ysoz_all = []\n",
    "            #print(idx)\n",
    "            \n",
    "            inputs = data['buffers'].long()\n",
    "            \n",
    "            B,Nsz,T, C, N = inputs.shape\n",
    "            chn_pos = torch.arange(C)\n",
    "            true_onset = data['onset map'].detach().numpy().reshape(-1)\n",
    "            #tot_sz += Nsz\n",
    "            for mcmc in range(n_samples):   \n",
    "                with torch.no_grad():\n",
    "                    try:\n",
    "                        _, psoz, _, _ = model(inputs)\n",
    "                    except:\n",
    "                        _, psoz, _, _ = model(inputs.double())\n",
    "                    ysoz_all.append(psoz.reshape(1, Nsz, C).detach().cpu().numpy())\n",
    "\n",
    "               \n",
    "            ysoz_all = np.concatenate(ysoz_all) #.reshape(tot_sz, 19)\n",
    "            #print(ysoz_all.shape)\n",
    "            for szn in range(Nsz):\n",
    "                \n",
    "                    ysoz, conf_y, max_chn_correct_y = final_loc(ysoz_all[:, szn, :].reshape(-1, 19), true_onset)\n",
    "                    corr_sz.append(max_chn_correct_y)\n",
    "                \n",
    "                    mc_var.append(conf_y)\n",
    "\n",
    "            #f, ax = plt.subplots(1,10, figsize = (20,2))\n",
    "            '''\n",
    "            ysoz, conf_y, max_chn_correct_y = final_loc(psoz[j,:,].reshape(1,-1),\n",
    "                                                            true_onset)\n",
    "            pos2d = topoplot(ysoz, ax[j],\n",
    "                                title='ysoz', zone=1,\n",
    "                                lobe_correct=max_chn_correct_y,\n",
    "                                lat_correct=max_chn_correct_y,\n",
    "                                onset_map=true_onset)\n",
    "            plt.show()\n",
    "            #print(Nsz, data['patient numbers'])\n",
    "            '''\n",
    "            del ysoz_all\n",
    "        del model\n",
    "        \n",
    "        print(modelname, cvfold, np.array(corr_sz).mean())\n",
    "        \n",
    "        locmodels[modelname]['szcorr'].append(np.array(corr_sz).mean())\n",
    "        locmodels[modelname]['szunc'].append(np.array(mc_var).mean())\n",
    "        del corr_sz, mc_var\n",
    "    del test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "768beeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonf = json.dumps(locmodels)\n",
    "\n",
    "# open file for writing, \"w\" \n",
    "f = open(\"loc_finetuned_results_final.json\",\"w\")\n",
    "\n",
    "# write json object to file\n",
    "f.write(jsonf)\n",
    "\n",
    "# close file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5877c246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
